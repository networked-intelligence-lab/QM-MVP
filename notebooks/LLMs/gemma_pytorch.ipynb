{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSioHoX63p7taBoy2TJNIx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/networked-intelligence-lab/QM-MVP/blob/main/notebooks/LLMs/gemma_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "pip install -q -U immutabledict sentencepiece\n",
        "git clone https://github.com/google/gemma_pytorch.git\n",
        "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/"
      ],
      "metadata": {
        "id": "bXqSQRwPUoe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca3SmmAPUh9_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/kaggle/working/gemma_pytorch/\")\n",
        "from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n",
        "from gemma.model import GemmaForCausalLM\n",
        "from gemma.tokenizer import Tokenizer\n",
        "import contextlib\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Load the model\n",
        "VARIANT = \"2b\"\n",
        "MACHINE_TYPE = \"cpu\"\n",
        "weights_dir = '/kaggle/input/gemma/pytorch/2b/2'\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def _set_default_tensor_type(dtype: torch.dtype):\n",
        "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
        "  torch.set_default_dtype(dtype)\n",
        "  yield\n",
        "  torch.set_default_dtype(torch.float)\n",
        "\n",
        "model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
        "model_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n",
        "\n",
        "device = torch.device(MACHINE_TYPE)\n",
        "with _set_default_tensor_type(model_config.get_dtype()):\n",
        "  model = GemmaForCausalLM(model_config)\n",
        "  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n",
        "  model.load_weights(ckpt_path)\n",
        "  model = model.to(device).eval()\n",
        "\n",
        "\n",
        "# Use the model\n",
        "\n",
        "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n",
        "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn>\\n\"\n",
        "\n",
        "prompt = (\n",
        "    USER_CHAT_TEMPLATE.format(\n",
        "        prompt=\"What is a good place for travel in the US?\"\n",
        "    )\n",
        "    + MODEL_CHAT_TEMPLATE.format(prompt=\"California.\")\n",
        "    + USER_CHAT_TEMPLATE.format(prompt=\"What can I do in California?\")\n",
        "    + \"<start_of_turn>model\\n\"\n",
        ")\n",
        "\n",
        "model.generate(\n",
        "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
        "    device=device,\n",
        "    output_len=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PUBaZ79ZUr11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}